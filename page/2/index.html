<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta name="google-site-verification" content="SEuuKcxiY0Z6R9aAPIZyU5Y9rFqpgO9gKES--zhFX24" />
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wangxchun.github.io","root":"/","images":"/images","scheme":"Pisces","version":"8.2.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"${hits} 找到 ${time} 個結果","hits":"找到 ${hits} 個結果"}};
  </script>
<meta name="description" content="機器學習 &#x2F; C++ &#x2F; 投資 &#x2F; 台灣景點">
<meta property="og:type" content="website">
<meta property="og:title" content="Nathan自强不息">
<meta property="og:url" content="https://wangxchun.github.io/page/2/index.html">
<meta property="og:site_name" content="Nathan自强不息">
<meta property="og:description" content="機器學習 &#x2F; C++ &#x2F; 投資 &#x2F; 台灣景點">
<meta property="og:locale" content="zh_TW">
<meta property="article:author" content="Nathan Wang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://wangxchun.github.io/page/2/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-TW'
  };
</script>
<title>Nathan自强不息</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Nathan自强不息</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Be Friend with Time</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首頁</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>關於</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分類</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>歸檔</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Nathan Wang"
      src="/img/avatar.jpg">
  <p class="site-author-name" itemprop="name">Nathan Wang</p>
  <div class="site-description" itemprop="description">機器學習 / C++ / 投資 / 台灣景點</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">226</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">303</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/wangxchun" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wangxchun" rel="noopener" target="_blank"><i class="GitHub fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:nathanwang0205@outlook.com" title="E-Mail → mailto:nathanwang0205@outlook.com" rel="noopener" target="_blank"><i class="e-mail fa-fw"></i>E-Mail</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/05/07/machine-learning-practice-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/07/machine-learning-practice-1/" class="post-title-link" itemprop="url">机器学习实战 --（1）regression</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-05-07 20:36:29" itemprop="dateCreated datePublished" datetime="2022-05-07T20:36:29+08:00">2022-05-07</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新於</span>
        <time title="修改時間：2022-05-08 11:11:38" itemprop="dateModified" datetime="2022-05-08T11:11:38+08:00">2022-05-08</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="一、前言"><a href="# 一、前言" class="headerlink" title="一、前言"></a> 一、前言 </h3><p>Objectives:</p>
<ul>
<li>Solve a regression problem with deep neural networks (DNN).</li>
<li>Understand basic DNN training tips.</li>
<li>Familiarize yourself with PyTorch.</li>
</ul>
<h3 id="二、引入 -python- 库"><a href="# 二、引入 -python- 库" class="headerlink" title="二、引入 python 库"></a> 二、引入 python 库 </h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;# Import packages&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Numerical Operations</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reading/Writing Data</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># For Progress Bar</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pytorch</span></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader, random_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># For plotting learning curve</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br></pre></td></tr></table></figure>
<h3 id="三、Utility-Functions"><a href="# 三、Utility-Functions" class="headerlink" title="三、Utility Functions"></a> 三、Utility Functions</h3><p> 设置 torch.backends.cudnn.benchmark=True 将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速。适用场景是网络结构固定（不是动态变化的），网络的输入形状（包括 batch size，图片大小，输入的通道）是不变的，其实也就是一般情况下都比较适用。反之，如果卷积层的设置一直变化，将会导致程序不停地做优化，反而会耗费更多的时间。</p>
<p>torch.backends.cudnn.deterministic=True<br> 每次返回的卷积算法将是确定的，即默认算法。cudnn 中包含很多卷积算法。基于 GEMM (General Matrix Multiply) 的，基于 FFT 的，基于 Winograd 算法的等等。</p>
<p> 设置 seed () 里的数字就相当于设置了一个盛有随机数的 “聚宝盆”，一个数字代表一个 “聚宝盆”，当我们在 seed（）的括号里设置相同的 seed，“聚宝盆” 就是一样的，从里面拿出的随机数就会相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rng</span> ():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">5</span>):</span><br><span class="line">        np.random.seed (<span class="number">123</span>)</span><br><span class="line">        <span class="built_in">print</span> (np.random.rand (<span class="number">4</span>))</span><br><span class="line">        </span><br><span class="line">rng ()</span><br><span class="line">&gt;&gt;&gt;[<span class="number">0.69646919</span> <span class="number">0.28613933</span> <span class="number">0.22685145</span> <span class="number">0.55131477</span>]</span><br><span class="line">   [<span class="number">0.69646919</span> <span class="number">0.28613933</span> <span class="number">0.22685145</span> <span class="number">0.55131477</span>]</span><br><span class="line">   [<span class="number">0.69646919</span> <span class="number">0.28613933</span> <span class="number">0.22685145</span> <span class="number">0.55131477</span>]</span><br><span class="line">   [<span class="number">0.69646919</span> <span class="number">0.28613933</span> <span class="number">0.22685145</span> <span class="number">0.55131477</span>]</span><br><span class="line">   [<span class="number">0.69646919</span> <span class="number">0.28613933</span> <span class="number">0.22685145</span> <span class="number">0.55131477</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rng_n</span> ():</span></span><br><span class="line">    np.random.seed (<span class="number">123</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span> (np.random.rand (<span class="number">4</span>))</span><br><span class="line">        </span><br><span class="line">rng_n ()</span><br><span class="line">&gt;&gt;&gt;[<span class="number">0.69646919</span> <span class="number">0.28613933</span> <span class="number">0.22685145</span> <span class="number">0.55131477</span>]</span><br><span class="line">   [<span class="number">0.71946897</span> <span class="number">0.42310646</span> <span class="number">0.9807642</span>  <span class="number">0.68482974</span>]</span><br><span class="line">   [<span class="number">0.4809319</span>  <span class="number">0.39211752</span> <span class="number">0.34317802</span> <span class="number">0.72904971</span>]</span><br><span class="line">   [<span class="number">0.43857224</span> <span class="number">0.0596779</span>  <span class="number">0.39804426</span> <span class="number">0.73799541</span>]</span><br><span class="line">   [<span class="number">0.18249173</span> <span class="number">0.17545176</span> <span class="number">0.53155137</span> <span class="number">0.53182759</span>]</span><br></pre></td></tr></table></figure>
<p> 在神经网络中，参数默认是进行随机初始化的。如果不设置的话每次训练时的初始化都是随机的，导致结果不确定。如果设置初始化，则每次初始化都是固定的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    random.seed (args.seed) <span class="comment">#</span></span><br><span class="line">    torch.manual_seed (args.seed)  <span class="comment"># 为 CPU 设置种子用于生成随机数，以使得结果是确定的   　　 torch.cuda.manual_seed (args.seed) #为当前 GPU 设置随机种子；</span></span><br><span class="line">    cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果使用多个 GPU，应该使用 torch.cuda.manual_seed_all () 为所有的 GPU 设置种子。</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.utils.data.random_split (dataset, lengths, generator=&lt;torch._C.Generator <span class="built_in">object</span>&gt;)</span><br></pre></td></tr></table></figure>
<p> 在 CPU 中设置生成随机数的种子，并返回一个 torch.Generator 对象。当设置的种子固定下来的时候，之后依次 pytorch 生成的随机数序列也被固定下来。需要注意的是当只调用 torch.manual_seed () 一次时并不能生成相同的随机数序列。如果想要得到相同的随机数序列就需要每次产生随机数的时候都要调用一下 torch.manual_seed ()。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.manual_seed (<span class="number">2</span>)</span><br><span class="line">&lt;torch._C.Generator <span class="built_in">object</span> at <span class="number">0x000001EB8F3A1918</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> (torch.randn (<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">tensor ([[<span class="number">0.3923</span>, -<span class="number">0.2236</span>],</span><br><span class="line">        [-<span class="number">0.3195</span>, -<span class="number">1.2050</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> (torch.randn (<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">tensor ([[<span class="number">1.0445</span>, -<span class="number">0.6332</span>],</span><br><span class="line">        [<span class="number">0.5731</span>,  <span class="number">0.5409</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/357075502">Pytorch：model.train () 和 model.eval () 用法和区别 </a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">same_seed</span> (<span class="params">seed</span>):</span> </span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Fixes random number generator seeds for reproducibility.&#x27;&#x27;&#x27;</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    np.random.seed (seed)</span><br><span class="line">    torch.manual_seed (seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available ():</span><br><span class="line">        torch.cuda.manual_seed_all (seed)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_valid_split</span> (<span class="params">data_set, valid_ratio, seed</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Split provided training data into training set and validation set&#x27;&#x27;&#x27;</span></span><br><span class="line">    valid_set_size = <span class="built_in">int</span> (valid_ratio * <span class="built_in">len</span> (data_set)) </span><br><span class="line">    train_set_size = <span class="built_in">len</span> (data_set) - valid_set_size</span><br><span class="line">    train_set, valid_set = random_split (data_set, [train_set_size, valid_set_size], generator=torch.Generator ().manual_seed (seed))</span><br><span class="line">    <span class="keyword">return</span> np.array (train_set), np.array (valid_set)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span> (<span class="params">test_loader, model, device</span>):</span></span><br><span class="line">    model.<span class="built_in">eval</span> () <span class="comment"># Set your model to evaluation mode.</span></span><br><span class="line">    preds = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> tqdm (test_loader):</span><br><span class="line">        x = x.to (device)                        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad ():                   </span><br><span class="line">            pred = model (x)                     </span><br><span class="line">            preds.append (pred.detach ().cpu ())   </span><br><span class="line">    preds = torch.cat (preds, dim=<span class="number">0</span>).numpy ()  </span><br><span class="line">    <span class="keyword">return</span> preds</span><br></pre></td></tr></table></figure>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 四、Dataset&quot;</span></span><br><span class="line">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;COVID19Dataset&lt;/span&gt; (&lt;span class=&quot;params&quot;&gt;Dataset&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;&amp;#x27;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    x: Features.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    y: Targets, if none, do prediction.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    &amp;#x27;&amp;#x27;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self, x, y=&lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; y &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.y = y&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.y = torch.FloatTensor (y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.x = torch.FloatTensor (x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__getitem__&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self, idx&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.y &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; self.x [idx]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; self.x [idx], self.y [idx]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__len__&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;len&lt;/span&gt; (self.x)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</span><br><span class="line">&lt;h3 id=&quot; 五、Neural-Network-Model&quot;&gt;&lt;a href=&quot;# 五、Neural-Network-Model&quot; class=&quot;headerlink&quot; title=&quot; 五、Neural Network Model&quot;&gt;&lt;/a&gt; 五、Neural Network Model&lt;/h3&gt;&lt;p&gt;Try out different model architectures by modifying the class below.&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;My_Model&lt;/span&gt; (&lt;span class=&quot;params&quot;&gt;nn.Module&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self, input_dim&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;built_in&quot;&gt;super&lt;/span&gt; (My_Model, self).__init__()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# &lt;span class=&quot;doctag&quot;&gt;TODO:&lt;/span&gt; modify model&amp;#x27;s structure, be aware of dimensions. &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.layers = nn.Sequential (&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            nn.Linear (input_dim, &lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            nn.ReLU (),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            nn.Linear (&lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            nn.ReLU (),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            nn.Linear (&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        )&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;forward&lt;/span&gt; (&lt;span class=&quot;params&quot;&gt;self, x&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = self.layers (x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = x.squeeze (&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;) &lt;span class=&quot;comment&quot;&gt;# (B, 1) -&amp;gt; (B)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; x&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;</span><br><span class="line">&lt;h3 id=&quot; 六、Feature-Selection&quot;&gt;&lt;a href=&quot;# 六、Feature-Selection&quot; class=&quot;headerlink&quot; title=&quot; 六、Feature Selection&quot;&gt;&lt;/a&gt; 六、Feature Selection&lt;/h3&gt;&lt;p&gt;Choose features you deem useful by modifying the function below.&lt;/p&gt;</span><br><span class="line">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;select_feat&lt;/span&gt; (&lt;span class=&quot;params&quot;&gt;train_data, valid_data, test_data, select_all=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;&amp;#x27;&amp;#x27;Selects useful features to perform regression&amp;#x27;&amp;#x27;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    y_train, y_valid = train_data [:,-&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;], valid_data [:,-&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    raw_x_train, raw_x_valid, raw_x_test = train_data [:,:-&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;], valid_data [:,:-&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;], test_data&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; select_all:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        feat_idx = &lt;span class=&quot;built_in&quot;&gt;list&lt;/span&gt; (&lt;span class=&quot;built_in&quot;&gt;range&lt;/span&gt; (raw_x_train.shape [&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        feat_idx = [&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;] &lt;span class=&quot;comment&quot;&gt;# &lt;span class=&quot;doctag&quot;&gt;TODO:&lt;/span&gt; Select suitable feature columns.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; raw_x_train [:,feat_idx], raw_x_valid [:,feat_idx], raw_x_test [:,feat_idx], y_train, y_valid&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</span><br><span class="line">&lt;h3 id=&quot; 六、Training-Loop&quot;&gt;&lt;a href=&quot;# 六、Training-Loop&quot; class=&quot;headerlink&quot; title=&quot; 六、Training Loop&quot;&gt;&lt;/a&gt; 六、Training Loop&lt;/h3&gt;&lt;p&gt;(1) batchsize：批大小。在深度学习中，一般采用 SGD 训练，即每次训练在训练集中取 batchsize 个样本训练；&lt;br&gt;(2) iteration：1 个 iteration 等于使用 batchsize 个样本训练一次；&lt;br&gt;(3) epoch：1 个 epoch 等于使用训练集中的全部样本训练一次；&lt;/p&gt;</span><br><span class="line">&lt;p&gt; 举个例子，训练集有 1000 个样本，batchsize=10，那么，&lt;br&gt; 训练完整个样本集需要：100 次 iteration，1 次 epoch。&lt;/p&gt;</span><br><span class="line">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;trainer&lt;/span&gt; (&lt;span class=&quot;params&quot;&gt;train_loader, valid_loader, model, config, device&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    criterion = nn.MSELoss (reduction=&lt;span class=&quot;string&quot;&gt;&amp;#x27;mean&amp;#x27;&lt;/span&gt;) &lt;span class=&quot;comment&quot;&gt;# Define your loss function, do not modify this.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# Define your optimization algorithm. &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# &lt;span class=&quot;doctag&quot;&gt;TODO:&lt;/span&gt; Please check https://pytorch.org/docs/stable/optim.html to get more available algorithms.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# &lt;span class=&quot;doctag&quot;&gt;TODO:&lt;/span&gt; L2 regularization (optimizer (weight decay...) or implement by your self).&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    optimizer = torch.optim.SGD (model.parameters (), lr=config [&lt;span class=&quot;string&quot;&gt;&amp;#x27;learning_rate&amp;#x27;&lt;/span&gt;], momentum=&lt;span class=&quot;number&quot;&gt;0.9&lt;/span&gt;) &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    writer = SummaryWriter () &lt;span class=&quot;comment&quot;&gt;# Writer of tensoboard.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; os.path.isdir (&lt;span class=&quot;string&quot;&gt;&amp;#x27;./models&amp;#x27;&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        os.mkdir (&lt;span class=&quot;string&quot;&gt;&amp;#x27;./models&amp;#x27;&lt;/span&gt;) &lt;span class=&quot;comment&quot;&gt;# Create directory of saving models.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    n_epochs, best_loss, step, early_stop_count = config [&lt;span class=&quot;string&quot;&gt;&amp;#x27;n_epochs&amp;#x27;&lt;/span&gt;], math.inf, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; epoch &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;range&lt;/span&gt; (n_epochs):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        model.train () &lt;span class=&quot;comment&quot;&gt;# Set your model to train mode.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        loss_record = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# tqdm is a package to visualize your training progress.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        train_pbar = tqdm (train_loader, position=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, leave=&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; x, y &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; train_pbar:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            optimizer.zero_grad ()               &lt;span class=&quot;comment&quot;&gt;# Set gradient to zero.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            x, y = x.to (device), y.to (device)   &lt;span class=&quot;comment&quot;&gt;# Move your data to device. &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            pred = model (x)             &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            loss = criterion (pred, y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            loss.backward ()                     &lt;span class=&quot;comment&quot;&gt;# Compute gradient (backpropagation).&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            optimizer.step ()                    &lt;span class=&quot;comment&quot;&gt;# Update parameters.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            step += &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            loss_record.append (loss.detach ().item ())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;comment&quot;&gt;# Display current epoch number and loss on tqdm progress bar.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            train_pbar.set_description (&lt;span class=&quot;string&quot;&gt;f&amp;#x27;Epoch [&lt;span class=&quot;subst&quot;&gt;&amp;#123;epoch+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&amp;#125;&lt;/span&gt;/&lt;span class=&quot;subst&quot;&gt;&amp;#123;n_epochs&amp;#125;&lt;/span&gt;]&amp;#x27;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            train_pbar.set_postfix (&amp;#123;&lt;span class=&quot;string&quot;&gt;&amp;#x27;loss&amp;#x27;&lt;/span&gt;: loss.detach ().item ()&amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        mean_train_loss = &lt;span class=&quot;built_in&quot;&gt;sum&lt;/span&gt; (loss_record)/&lt;span class=&quot;built_in&quot;&gt;len&lt;/span&gt; (loss_record)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        writer.add_scalar (&lt;span class=&quot;string&quot;&gt;&amp;#x27;Loss/train&amp;#x27;&lt;/span&gt;, mean_train_loss, step)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        model.&lt;span class=&quot;built_in&quot;&gt;eval&lt;/span&gt; () &lt;span class=&quot;comment&quot;&gt;# Set your model to evaluation mode.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        loss_record = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; x, y &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; valid_loader:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            x, y = x.to (device), y.to (device)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;with&lt;/span&gt; torch.no_grad ():&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                pred = model (x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                loss = criterion (pred, y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            loss_record.append (loss.item ())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        mean_valid_loss = &lt;span class=&quot;built_in&quot;&gt;sum&lt;/span&gt; (loss_record)/&lt;span class=&quot;built_in&quot;&gt;len&lt;/span&gt; (loss_record)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt; (&lt;span class=&quot;string&quot;&gt;f&amp;#x27;Epoch [&lt;span class=&quot;subst&quot;&gt;&amp;#123;epoch+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&amp;#125;&lt;/span&gt;/&lt;span class=&quot;subst&quot;&gt;&amp;#123;n_epochs&amp;#125;&lt;/span&gt;]: Train loss: &lt;span class=&quot;subst&quot;&gt;&amp;#123;mean_train_loss:&lt;span class=&quot;number&quot;&gt;.4&lt;/span&gt;f&amp;#125;&lt;/span&gt;, Valid loss: &lt;span class=&quot;subst&quot;&gt;&amp;#123;mean_valid_loss:&lt;span class=&quot;number&quot;&gt;.4&lt;/span&gt;f&amp;#125;&lt;/span&gt;&amp;#x27;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        writer.add_scalar (&lt;span class=&quot;string&quot;&gt;&amp;#x27;Loss/valid&amp;#x27;&lt;/span&gt;, mean_valid_loss, step)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; mean_valid_loss &amp;lt; best_loss:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            best_loss = mean_valid_loss&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            torch.save (model.state_dict (), config [&lt;span class=&quot;string&quot;&gt;&amp;#x27;save_path&amp;#x27;&lt;/span&gt;]) &lt;span class=&quot;comment&quot;&gt;# Save your best model&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt; (&lt;span class=&quot;string&quot;&gt;&amp;#x27;Saving model with loss &amp;#123;:.3f&amp;#125;...&amp;#x27;&lt;/span&gt;.&lt;span class=&quot;built_in&quot;&gt;format&lt;/span&gt; (best_loss))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            early_stop_count = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;: &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            early_stop_count += &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; early_stop_count &amp;gt;= config [&lt;span class=&quot;string&quot;&gt;&amp;#x27;early_stop&amp;#x27;&lt;/span&gt;]:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt; (&lt;span class=&quot;string&quot;&gt;&amp;#x27;\nModel is not improving, so we halt the training session.&amp;#x27;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</span><br><span class="line">&lt;h2 id=&quot; 七、Configurations&quot;&gt;&lt;a href=&quot;# 七、Configurations&quot; class=&quot;headerlink&quot; title=&quot; 七、Configurations&quot;&gt;&lt;/a&gt; 七、Configurations&lt;/h2&gt;&lt;p&gt;&lt;code&gt;config&lt;/code&gt; contains hyper-parameters for training and the path to save your model.&lt;/p&gt;</span><br><span class="line">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;device = &lt;span class=&quot;string&quot;&gt;&amp;#x27;cuda&amp;#x27;&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; torch.cuda.is_available () &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&amp;#x27;cpu&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;config = &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;seed&amp;#x27;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;5201314&lt;/span&gt;,      &lt;span class=&quot;comment&quot;&gt;# Your seed number, you can pick your lucky number. :)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;select_all&amp;#x27;&lt;/span&gt;: &lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;,   &lt;span class=&quot;comment&quot;&gt;# Whether to use all features.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;valid_ratio&amp;#x27;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;0.2&lt;/span&gt;,   &lt;span class=&quot;comment&quot;&gt;# validation_size = train_size * valid_ratio&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;n_epochs&amp;#x27;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;3000&lt;/span&gt;,     &lt;span class=&quot;comment&quot;&gt;# Number of epochs.            &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;batch_size&amp;#x27;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;learning_rate&amp;#x27;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;1e-5&lt;/span&gt;,              &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;early_stop&amp;#x27;&lt;/span&gt;: &lt;span class=&quot;number&quot;&gt;400&lt;/span&gt;,    &lt;span class=&quot;comment&quot;&gt;# If model has not improved for this many consecutive epochs, stop training.     &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;save_path&amp;#x27;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&amp;#x27;./models/model.ckpt&amp;#x27;&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# Your model will be saved here.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</span><br><span class="line">&lt;p&gt;“””<span class="comment"># Dataloader&lt;br&gt;Read data from files and set up training, validation, and testing sets. You do not need to modify this part.&lt;br&gt;“””&lt;/p&gt;</span></span><br><span class="line">&lt;h1 id=&quot;Set-seed-for-reproducibility&quot;&gt;&lt;a href=&quot;#Set-seed-for-reproducibility&quot; class=&quot;headerlink&quot; title=&quot;Set seed for reproducibility&quot;&gt;&lt;/a&gt;Set seed for reproducibility&lt;/h1&gt;&lt;p&gt;same_seed (config [‘seed’])&lt;/p&gt;</span><br><span class="line">&lt;h1 id=&quot;train-data-size-2699-x-118-id-37-states-16-features-x-5-days&quot;&gt;&lt;a href=&quot;#train-data-size-2699-x-118-id-37-states-16-features-x-5-days&quot; class=&quot;headerlink&quot; title=&quot;train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days)&quot;&gt;&lt;/a&gt;train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days)&lt;/h1&gt;&lt;h1 id=&quot;test-data-size-1078-x-117-without-last-day’s-positive-rate&quot;&gt;&lt;a href=&quot;#test-data-size-1078-x-117-without-last-day’s-positive-rate&quot; class=&quot;headerlink&quot; title=&quot;test_data size: 1078 x 117 (without last day’s positive rate)&quot;&gt;&lt;/a&gt;test_data size: 1078 x 117 (without last day’s positive rate)&lt;/h1&gt;&lt;p&gt;train_data, test_data = pd.read_csv (‘./covid.train.csv’).values, pd.read_csv (‘./covid.test.csv’).values&lt;br&gt;train_data, valid_data = train_valid_split (train_data, config [‘valid_ratio’], config [‘seed’])&lt;/p&gt;</span><br><span class="line">&lt;h1 id=&quot;Print-out-the-data-size&quot;&gt;&lt;a href=&quot;#Print-out-the-data-size&quot; class=&quot;headerlink&quot; title=&quot;Print out the data size.&quot;&gt;&lt;/a&gt;Print out the data size.&lt;/h1&gt;&lt;p&gt;print (f”””train_data size: &#123;train_data.shape&#125;&lt;br&gt;valid_data size: &#123;valid_data.shape&#125;&lt;br&gt;test_data size: &#123;test_data.shape&#125;”””)&lt;/p&gt;</span><br><span class="line">&lt;h1 id=&quot;Select-features&quot;&gt;&lt;a href=&quot;#Select-features&quot; class=&quot;headerlink&quot; title=&quot;Select features&quot;&gt;&lt;/a&gt;Select features&lt;/h1&gt;&lt;p&gt;x_train, x_valid, x_test, y_train, y_valid = select_feat (train_data, valid_data, test_data, config [‘select_all’])&lt;/p&gt;</span><br><span class="line">&lt;h1 id=&quot;Print-out-the-number-of-features&quot;&gt;&lt;a href=&quot;#Print-out-the-number-of-features&quot; class=&quot;headerlink&quot; title=&quot;Print out the number of features.&quot;&gt;&lt;/a&gt;Print out the number of features.&lt;/h1&gt;&lt;p&gt;print (f’number of features: &#123;x_train.shape [1]&#125;’)&lt;/p&gt;</span><br><span class="line">&lt;p&gt;train_dataset, valid_dataset, test_dataset = COVID19Dataset (x_train, y_train), \&lt;br&gt;                                            COVID19Dataset (x_valid, y_valid), \&lt;br&gt;                                            COVID19Dataset (x_test)&lt;/p&gt;</span><br><span class="line">&lt;h1 id=&quot;Pytorch-data-loader-loads-pytorch-dataset-into-batches&quot;&gt;&lt;a href=&quot;#Pytorch-data-loader-loads-pytorch-dataset-into-batches&quot; class=&quot;headerlink&quot; title=&quot;Pytorch data loader loads pytorch dataset into batches.&quot;&gt;&lt;/a&gt;Pytorch data loader loads pytorch dataset into batches.&lt;/h1&gt;&lt;p&gt;train_loader = DataLoader (train_dataset, batch_size=config [‘batch_size’], shuffle=True, pin_memory=True)&lt;br&gt;valid_loader = DataLoader (valid_dataset, batch_size=config [‘batch_size’], shuffle=True, pin_memory=True)&lt;br&gt;test_loader = DataLoader (test_dataset, batch_size=config [‘batch_size’], shuffle=False, pin_memory=True)&lt;/p&gt;</span><br></pre></td></tr></table></figure>
<h3 id="Start-training"><a href="#Start-training" class="headerlink" title="Start training!"></a>Start training!</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model = My_Model (input_dim=x_train.shape [<span class="number">1</span>]).to (device) <span class="comment"># put your model and data on the same computation device.</span></span><br><span class="line">trainer (train_loader, valid_loader, model, config, device)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Plot-learning-curves-with-tensorboard-optional"><a href="#Plot-learning-curves-with-tensorboard-optional" class="headerlink" title="Plot learning curves with tensorboard (optional)"></a>Plot learning curves with <code>tensorboard</code> (optional)</h3><h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p>The predictions of your model on testing set will be stored at <code>pred.csv</code>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_pred</span> (<span class="params">preds, file</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Save predictions to specified file &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span> (file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        writer = csv.writer (fp)</span><br><span class="line">        writer.writerow ([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tested_positive&#x27;</span>])</span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span> (preds):</span><br><span class="line">            writer.writerow ([i, p])</span><br><span class="line"></span><br><span class="line">model = My_Model (input_dim=x_train.shape [<span class="number">1</span>]).to (device)</span><br><span class="line">model.load_state_dict (torch.load (config [<span class="string">&#x27;save_path&#x27;</span>]))</span><br><span class="line">preds = predict (test_loader, model, device) </span><br><span class="line">save_pred (preds, <span class="string">&#x27;pred.csv&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>This notebook uses code written by Heng-Jui Chang @ NTUEE (<a target="_blank" rel="noopener" href="https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb">https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb</a>)</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/02/17/bishe-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/17/bishe-3/" class="post-title-link" itemprop="url">bishe-3</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>
      

      <time title="創建時間：2022-02-17 21:26:43 / 修改時間：21:47:53" itemprop="dateCreated datePublished" datetime="2022-02-17T21:26:43+08:00">2022-02-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/02/14/bishe-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/14/bishe-2/" class="post-title-link" itemprop="url">毕业设计 -(2) 知识学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-02-14 12:50:25" itemprop="dateCreated datePublished" datetime="2022-02-14T12:50:25+08:00">2022-02-14</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新於</span>
        <time title="修改時間：2022-02-17 21:50:02" itemprop="dateModified" datetime="2022-02-17T21:50:02+08:00">2022-02-17</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/C-advance/" itemprop="url" rel="index"><span itemprop="name">C++ advance</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="一、深度学习基础"><a href="#一、深度学习基础" class="headerlink" title="一、深度学习基础"></a>一、深度学习基础</h3><h4 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h4><p>(1) 领域专家，数据科学家，AI 专家<br>Q：数据科学家和 AI 专家的区别？<br>数据科学家可以有两条职业规划<br>广：不断开拓模型在不同领域的应用<br>深：专攻一个领域，成为该领域的专家<br>(2) <a target="_blank" rel="noopener" href="https://kopu.chat/2012-deeplearning-nvidia-gpu/">2012 年令深度學習和 NVIDIA 股價火爆起來的真正關鍵──GPU</a><br>(3) 模型的可解释性<br>(4) 符号学可以和深度学习结合吗？可以，例如图神经网络<br>(5) 学习的阶段：看 -&gt; 听 -&gt; 看 + 听 -&gt; 动手做 -&gt; 讲解一遍给自己或别人听</p>
<h4 id="2-深度学习基础-线性神经网络，多层感知器"><a href="#2-深度学习基础-线性神经网络，多层感知器" class="headerlink" title="2. 深度学习基础 - 线性神经网络，多层感知器"></a>2. 深度学习基础 - 线性神经网络，多层感知器</h4><h4 id="3-卷积神经网络-LeNet-Alexnet-VGG-Inception-Resnet"><a href="#3-卷积神经网络-LeNet-Alexnet-VGG-Inception-Resnet" class="headerlink" title="3. 卷积神经网络 - LeNet, Alexnet, VGG, Inception, Resnet"></a>3. 卷积神经网络 - LeNet, Alexnet, VGG, Inception, Resnet</h4><h4 id="4-循环神经网络-RNN-GRU-LSTM-seq2seq"><a href="#4-循环神经网络-RNN-GRU-LSTM-seq2seq" class="headerlink" title="4. 循环神经网络 - RNN, GRU, LSTM, seq2seq"></a>4. 循环神经网络 - RNN, GRU, LSTM, seq2seq</h4><h4 id="5-注意力机制-Attention-Transformer"><a href="#5-注意力机制-Attention-Transformer" class="headerlink" title="5. 注意力机制 - Attention, Transformer"></a>5. 注意力机制 - Attention, Transformer</h4><h4 id="6-优化算法-SGD-Momentum-Adam"><a href="#6-优化算法-SGD-Momentum-Adam" class="headerlink" title="6. 优化算法 - SGD, Momentum, Adam"></a>6. 优化算法 - SGD, Momentum, Adam</h4><h4 id="7-高性能算法-并行，多-GPU-分布式"><a href="#7-高性能算法-并行，多-GPU-分布式" class="headerlink" title="7. 高性能算法 -  并行，多 GPU, 分布式"></a>7. 高性能算法 -  并行，多 GPU, 分布式</h4><h4 id="8-计算机视觉-目标检测，语义分析"><a href="#8-计算机视觉-目标检测，语义分析" class="headerlink" title="8. 计算机视觉 - 目标检测，语义分析"></a>8. 计算机视觉 - 目标检测，语义分析</h4><h4 id="9-自然语言处理-词嵌入，BERT"><a href="#9-自然语言处理-词嵌入，BERT" class="headerlink" title="9. 自然语言处理 - 词嵌入，BERT"></a>9. 自然语言处理 - 词嵌入，BERT</h4>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/02/14/bishe-2/#more" rel="contents">
                閱讀全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/02/14/bishe-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/14/bishe-1/" class="post-title-link" itemprop="url">毕业设计 -(1) 前期准备</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-02-14 12:42:08" itemprop="dateCreated datePublished" datetime="2022-02-14T12:42:08+08:00">2022-02-14</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新於</span>
        <time title="修改時間：2022-02-17 01:08:40" itemprop="dateModified" datetime="2022-02-17T01:08:40+08:00">2022-02-17</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/C-advance/" itemprop="url" rel="index"><span itemprop="name">C++ advance</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343592923">PyCharm+Anaconda+CUDA+Pytorch 安装教程（个人经验帖）</a></p>
<h4 id="1-工具的作用"><a href="#1-工具的作用" class="headerlink" title="1. 工具的作用"></a>1. 工具的作用</h4><h5 id="1-PyCharm"><a href="#1-PyCharm" class="headerlink" title="(1) PyCharm"></a>(1) PyCharm</h5><p>PyCharm 是一个 Python 语言的一个开发工具，类似于 Eclipse（Java）</p>
<h5 id="2-Anaconda"><a href="#2-Anaconda" class="headerlink" title="(2) Anaconda"></a>(2) Anaconda</h5><p>Anaconda 里面集成了很多关于 python 科学计算的第三方库。我们安装了它，就安装了很多我们需要用到的工具包</p>
<h5 id="3-CUDA（Compute-Unified-Device-Architecture"><a href="#3-CUDA（Compute-Unified-Device-Architecture" class="headerlink" title="(3) CUDA（Compute Unified Device Architecture"></a>(3) CUDA（Compute Unified Device Architecture</h5><p>CUDA 是显卡厂商 NVIDIA 推出的运算平台，可以针对 GPU 做加速神经网络计算。<br>CUDA™是一种由 NVIDIA 推出的通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题。<br>cudnn 是 pytorch 搭建深度学习模型的依赖，没有它，不能运行卷积等操作。</p>
<h5 id="4-PyTorch-是一个开源的-Python-机器学习库，不仅能够实现强大的-GPU-加速，同时还支持动态神经网络"><a href="#4-PyTorch-是一个开源的-Python-机器学习库，不仅能够实现强大的-GPU-加速，同时还支持动态神经网络" class="headerlink" title="(4) PyTorch 是一个开源的 Python 机器学习库，不仅能够实现强大的 GPU 加速，同时还支持动态神经网络"></a>(4) PyTorch 是一个开源的 Python 机器学习库，不仅能够实现强大的 GPU 加速，同时还支持动态神经网络</h5><h4 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h4><h5 id="1-安装-Anaconda"><a href="#1-安装-Anaconda" class="headerlink" title="(1) 安装 Anaconda"></a>(1) 安装 Anaconda</h5><h5 id="2-打开-Anaconda-Prompt"><a href="#2-打开-Anaconda-Prompt" class="headerlink" title="(2) 打开 Anaconda Prompt"></a>(2) 打开 Anaconda Prompt</h5><h5 id="3-显卡配置（没有独立显卡，略）"><a href="#3-显卡配置（没有独立显卡，略）" class="headerlink" title="(3) 显卡配置（没有独立显卡，略）"></a>(3) 显卡配置（没有独立显卡，略）</h5><p>CUDN 安装<br>cuDNN 下载</p>
<h5 id="4-激活环境"><a href="#4-激活环境" class="headerlink" title="(4) 激活环境"></a>(4) 激活环境</h5><p>检查 Python 版本：在 cmd 输入 python<br>管理环境<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 Anaconda Prompt 中输入</span></span><br><span class="line">conda create -n pytorch python=<span class="number">3.7</span></span><br><span class="line">conda activate pytorch</span><br></pre></td></tr></table></figure></p>
<h5 id="5-安装-pytorch"><a href="#5-安装-pytorch" class="headerlink" title="(5) 安装 pytorch"></a>(5) 安装 pytorch</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 到 pytorch 官网查询安装指令</span></span><br><span class="line">conda install pytorch torchvision cpuonly -c pytorch</span><br><span class="line"><span class="comment"># -c 参数指明了下载 pytorch 的通道，优先级比清华镜像更高</span></span><br><span class="line"><span class="comment">#（如果你有更改镜像源，会被盖过去）</span></span><br></pre></td></tr></table></figure>
<h5 id="6-验证-PyTorch-安装成功"><a href="#6-验证-PyTorch-安装成功" class="headerlink" title="(6) 验证 PyTorch 安装成功"></a>(6) 验证 PyTorch 安装成功</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 在 Anaconda Prompt 中的 pytorch 环境中</span><br><span class="line"># 输入 python 进到 python 界面</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvison</span><br><span class="line"><span class="built_in">print</span> (torch)</span><br></pre></td></tr></table></figure>
<h4 id="3"><a href="#3" class="headerlink" title="3."></a>3.</h4><h5 id="1"><a href="#1" class="headerlink" title="(1)"></a>(1)</h5><h5 id="2"><a href="#2" class="headerlink" title="(2)"></a>(2)</h5><h5 id="3-1"><a href="#3-1" class="headerlink" title="(3)"></a>(3)</h5><h5 id="4"><a href="#4" class="headerlink" title="(4)"></a>(4)</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>
<p><a href>Windows10 系统中配置安装 PyTorch 环境，无显卡配置</a><br></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/02/14/bishe-1/#more" rel="contents">
                閱讀全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/01/16/quick-read-wenxue-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/16/quick-read-wenxue-1/" class="post-title-link" itemprop="url">西方古典文学 -《神曲》（但丁）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>
      

      <time title="創建時間：2022-01-16 00:13:59 / 修改時間：23:11:24" itemprop="dateCreated datePublished" datetime="2022-01-16T00:13:59+08:00">2022-01-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%B8%BB%E9%A2%98%E4%B9%A6%E7%B1%8D%E9%80%9F%E8%AF%BB-%E6%96%87%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">主题书籍速读-文学</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="介绍"><a href="# 介绍" class="headerlink" title="介绍"></a>介绍 </h3><h4 id="作者"><a href="# 作者" class="headerlink" title="作者"></a> 作者 </h4><p> 但丁</p>
<ul>
<li>中世纪伟大诗人</li>
<li>文艺复兴的先驱</li>
<li>意大利语之父</li>
</ul>
<p>如果说西方文学史的殿堂只能保留两个名字，那只能是但丁和莎士比亚。但丁对后世的影响极为深远，我们中国的文化进程也深受其影响。比如，他曾激励戊戌变法失败后的梁启超，让在日本留学的鲁迅看到了用语言凝聚民族的希望，而发起 “白话文运动” 的胡适，效仿的正是但丁对意大利语的创造和革新。</p>
<p>但丁是用他惊人的原创力征服世界的。他发明了意大利语。但丁出生于 1265 年的意大利，当时意大利并不是一个统一的国家，只是一个地理概念。当时的欧洲通用语是拉丁文，各地老百姓说的语言被称为 “俗语”，英语、法语、意大利语，都是俗语。文人著书立说用的都是拉丁文，因为这种语言被认为是神圣和不朽的。拉丁文和俗语的关系，很像我们新文化运动前文言文和白话文的关系。</p>
<h4 id="作品"><a href="# 作品" class="headerlink" title="作品"></a>作品</h4><p>《神曲》</p>
<ul>
<li>一部百科全书式作品</li>
<li>一册不朽诗篇</li>
</ul>
<p>简单地说，《神曲》写的是神游三界的故事：朝圣者但丁在人生旅途上迷失了，陷入了一片黑暗森林，找不到出路，这时，罗马大诗人维吉尔出现了，他和一个叫贝亚特丽斯的圣女带领但丁游历了地狱、炼狱和天堂，最终见到了上帝。</p>
<p>几个世纪以来，几乎每个重视教育的西方家庭中都有一本《神曲》，一般来说都是多雷的版画插图珍藏版，孩子们从小就阅读甚至背诵它，被其中或可怖或怪诞或恢宏的景象所征服。但其实，它实在是一本极为复杂的书。它是一部百科全书式作品，囊括了中世纪所有的人文知识，有着把整个宇宙囊括进去的野心。它写了一百多个人物，从地狱一路铺到天堂，这一百多个灵魂的故事，贯穿了从古希腊罗马到但丁生活的中世纪尾页的历史人物、神话人物、文学人物和与但丁同时代的真实人物。</p>
<h3 id="创作神曲的原动力"><a href="# 创作神曲的原动力" class="headerlink" title="创作神曲的原动力"></a>创作神曲的原动力 </h3><h4 id="贝亚特丽斯是最重要的事"><a href="# 贝亚特丽斯是最重要的事" class="headerlink" title="贝亚特丽斯是最重要的事"></a> 贝亚特丽斯是最重要的事 </h4><p> 美德美人 -&gt; 精神苦恋 -&gt; 私人神话 -&gt; 永恒纪念</p>
<h4 id="被佛罗伦萨放逐开始流亡"><a href="# 被佛罗伦萨放逐开始流亡" class="headerlink" title="被佛罗伦萨放逐开始流亡"></a>被佛罗伦萨放逐开始流亡 </h4><p> 政治斗争 -&gt; 放逐流亡 -&gt; 独特观点 -&gt; 创作史诗</p>
<p>那时候，流亡是对一个人最严厉的惩罚。因为在中世纪的观念中，一个人在城邦里占据的位置是他最重要的东西，不能占有城市中的一个位置，就意味着你什么也不是。这对一个致力于世俗生活的普通人来说是惩罚，但对一个诗人来说却是一种幸福。</p>
<p>开始流亡后，但丁彻底抛弃了忠于哪种党派的观念，开始拥有自己的独特观点。他开始了语言上的研究，试图找到一个统一的、可以综合所有意大利语言的可能，来描述以君主帝国来统一世界的必要性。他开始继续写作《神曲》，他的生命也因此重新绽放。这一次，他的创作从早年私人的爱情神话变成了属于公众的神话、史诗和百科全书。</p>
<h3 id="神曲开篇"><a href="# 神曲开篇" class="headerlink" title="神曲开篇"></a>神曲开篇 </h3><p> 古罗马最伟大的诗人罗吉尔，他的灵活作为但丁的导师</p>
<p>神曲是有韵的诗，开篇的韵脚是由 vita（也就是 “生命” 这个词）和 smarrita（也就是 “迷失”）所构成的，故事的开局就是 “迷失的生命”。主人公是朝圣者但丁，一个迷途的普通人，而叙述者是全知全能的作者但丁，我们必须把这两者区分开。按照当时人对寿命的理解，人的寿命最长大约 70 岁，那么人生的中途就是 35 岁，一个容易产生中年危机的年龄。</p>
<p>开篇有一系列象征和用典。黑暗森林象征人类精神世界的种种罪恶和过失，也指当时的意大利社会的腐败和党派之争。三只猛兽来源于圣经中的典故，在《启示录》中代表 “反基督”，因为豹子（lonza）、狮子（leone）、狼（lupa）的意大利文都以字母 “l” 开头，这正是魔鬼路西法（Lucifer）名字的起首字母。豹象征的是肉欲和享乐，在政治上指的是将但丁驱逐出去的佛罗伦萨，狮子象征野心和强权，在政治上指法兰西国王出兵意大利，母狼象征贪欲，在政治上指的是贪婪的罗马教皇。在《地狱篇》第 2 歌中，但丁借维吉尔之口预言说，一只猎犬将成为意大利的救星，将象征贪欲的母狼赶回地狱，这只猎犬以 “智慧、爱和美德” 为食，指的是当时但丁给予厚望的君主：神圣罗马帝国的皇帝亨利七世。</p>
<p>你看，这里的象征都是具有双重含义的，既是在说人生面临的困境，也是在说意大利面临的困境，而但丁的写作意图也是双重的，他要讲述自己摆脱人性的诱惑，走出人生困境的旅程，也希望意大利能扫除罪恶，得到拯救。其实，《神曲》讲述的就是但丁在流亡期间，游走在不同城市之间，作为一个旁观者，深刻地发现意大利的真正问题所在的故事，他讲述自己对这个四分五裂的世界的认知，告诉人们如何才能齐心协力，从分裂走向整合。</p>
<h3 id="朝圣者但丁的新三界"><a href="# 朝圣者但丁的新三界" class="headerlink" title="朝圣者但丁的新三界"></a>朝圣者但丁的新三界 </h3><h4 id="地狱"><a href="# 地狱" class="headerlink" title="地狱"></a> 地狱 </h4><p> 地狱最外围的一层叫灵泊，limbo，这里漂浮着古代圣贤的灵魂，其中有哲学家苏格拉底、诗人荷马，还有但丁诗歌上的老师维吉尔等等。你可能要问，他们有什么罪呢？</p>
<p>按照但丁的神学观念，古代圣贤生活在基督诞生之前，当然不可能有基督教信仰，所以，他们不能得到救赎，永远悬在半空，地狱不罚他们，天堂也不要他们。正像维吉尔说的，他们既不期盼，也不等候，不抱希望地存在着。所以，这其实也是一种惩罚，因为这种状态就是被放逐。但丁对灵泊的创造很可能来源于自己被放逐的经历。不同的是，但丁始终是怀有希望的。如今 limbo 早已成为一个常用词，用来指这种边缘和放逐的状态。比如电影《盗梦空间》中的 limbo，“迷失域”，指的就是潜意识的边缘，一种没有构建、没有秩序、仅有潜意识投射的状态。人一旦进入到这里，就会忘了自己在做梦，也忘了时间，这无疑是一种放逐。</p>
<p>我们再沿着《神曲》中的地狱往下走。真正的地狱开始于第二圈，这里住着色欲场中的灵魂，在狂风中飘荡；第三圈是犯了饕餮罪的，躺在臭雨冰雹之下，第四圈是贪婪浪费者，永远推着重物上山，互相冲撞；第五圈是愤怒者，他们的灵魂在死的河里争斗。地狱前五圈的罪基本对应的是 “七宗罪”：骄傲，妒忌，愤怒，懒惰，贪财，贪吃，贪色，第五圈之下是 “下层地狱”，这里的罪出自但丁的原创，因此分类也更详细。第六圈是不信灵魂存在的邪教徒。第七圈是残暴者，包括对他人施暴的人和对自己施暴的人也就是自杀者，以及对上帝残暴的人和对自然残暴的人等等。但丁的老师拉蒂尼就被安排在这一圈，学者认为，但丁给老师安排的罪名不太有说服力，他这么安排很可能只是为了凸显自己在道德上比老师更高尚。第八圈是欺诈者，包括但丁痛恨的贪官污吏、伪君子等，著名的尤利西斯的灵魂也在这里。第九圈是一个冰湖，罪人都冰冻在里面，中心是万恶所归的魔王路西法，就是撒旦。</p>
<p>爱欲问题是西方文化的关键词之一，很多学者认为，爱欲问题也是但丁的 “全部问题”。这里的爱是希腊语 Eros 也就是 “爱欲” 的爱，它是一种生命动力，类似于 “力比多” 和 “自由意志”。</p>
<p>在但丁心里，狂妄的求知欲是一种很深的罪。作为一个百科全书式的求知者，但丁是矛盾的。一方面，他像笔下的尤利西斯一样，想要穷尽一切知识，另一方面，按照他的信仰，求知欲始于过度的好奇，来自人对超越自身限度的善的渴望，而这种渴望是一种傲慢，傲慢是最深的罪。在圣经故事中，人类的始祖亚当夏娃正是因为过度的好奇而吃下智慧树的果子，有了原罪。对未知世界，正确的态度是保持几分审慎和谦卑。</p>
<h4 id="炼狱"><a href="# 炼狱" class="headerlink" title="炼狱"></a>炼狱 </h4><p> 炼狱是一座山，形状跟地狱正相反，像一个倒立的漏斗，加上净界山和地上乐园，也是 9 层，越往上越接近天堂。炼狱的灵魂犯下的也是七宗罪，但跟地狱的灵魂不同的是，他们知道忏悔，还有得救的希望。</p>
<p>如果说《地狱篇》展示的是各种堕落的爱，《炼狱篇》展示的就是人用自由意志对爱欲的纠正，对爱之罪的医治。自由意志是《炼狱篇》重点讨论的关键词。</p>
<h4 id="天堂"><a href="# 天堂" class="headerlink" title="天堂"></a>天堂</h4><p>《天堂篇》所展示的，是爱的完满状态，这里居住着幸福的灵魂，他们是行善者、虔诚的教士、为信仰而战死的英雄、哲学家和神学家、正直的君主，基督和天使。</p>
<p>宇宙运转的真理 - 爱 <br> 这种爱，是基督教中的仁爱（caritas），或者说信仰之爱。</p>
<p>爱的完满状态就是兼具人性与神性，调和了尤利西斯的求知之爱，维吉尔的理智之爱，贝亚特丽斯的信仰之爱的状态。这就是但丁关于爱的理想，是他对自己灵魂进阶的要求，也是他为纷乱的人世间开出的药方。</p>
<h3 id="延伸阅读 -《月亮与六便士》-《千面英雄》-《荷马史诗》"><a href="# 延伸阅读 -《月亮与六便士》-《千面英雄》-《荷马史诗》" class="headerlink" title="延伸阅读 《月亮与六便士》 《千面英雄》 《荷马史诗》"></a>延伸阅读 《月亮与六便士》 《千面英雄》 《荷马史诗》</h3>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/01/15/quick-read-kehuan-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/15/quick-read-kehuan-1/" class="post-title-link" itemprop="url">《仿生人会梦见电子羊吗》</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-01-15 17:54:36" itemprop="dateCreated datePublished" datetime="2022-01-15T17:54:36+08:00">2022-01-15</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新於</span>
        <time title="修改時間：2022-01-16 23:11:24" itemprop="dateModified" datetime="2022-01-16T23:11:24+08:00">2022-01-16</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%B8%BB%E9%A2%98%E4%B9%A6%E7%B1%8D%E9%80%9F%E8%AF%BB-%E7%A7%91%E5%B9%BB/" itemprop="url" rel="index"><span itemprop="name">主题书籍速读-科幻</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="仿生人的悲剧"><a href="# 仿生人的悲剧" class="headerlink" title="仿生人的悲剧"></a>仿生人的悲剧 </h3><p> 外在环境的压力迫使人类文明趋向于创造出超越人类自身能力的造物，并把它们作为工具，而随着造物不断接近人类本体，双方必然产生无法调和的矛盾冲突。</p>
<h3 id="仿生人悲剧的必然"><a href="# 仿生人悲剧的必然" class="headerlink" title="仿生人悲剧的必然"></a>仿生人悲剧的必然 </h3><p> 人类必然需要为自己的造物上一层保险，限制它们的超级能力反过来危害人类，但同时这种限制成为检验人性善恶的一面镜子，将引起人类文明伦理道德基准的反思与动摇。</p>
<p>工具必然会接近并超越人类 vs 工具天然处于被支配地位 人类对工具严格限制<br>-&gt; 矛盾造成悲剧</p>
<h3 id="推论与现实对照"><a href="# 推论与现实对照" class="headerlink" title="推论与现实对照"></a>推论与现实对照 </h3><h4 id="追求强大工具的客观现实必要性"><a href="# 追求强大工具的客观现实必要性" class="headerlink" title="追求强大工具的客观现实必要性"></a> 追求强大工具的客观现实必要性 </h4><p> 这篇小说的设定里，作者采用了一个近未来后启示录背景，也就是《圣经》中启示录的末日预警之后。这种设定在科幻小说里很常见，巨大的灾难比如核战争，毁掉了地球，人类进入了极端残酷的生存环境。这样设定带来最重要的点，就是故事中的人类面临的生存压力非常大。在巨大的压力下，各种东西都会被推到极端，很多温情脉脉的面纱都会揭掉，露出最残忍的人性本性来。</p>
<p>我们可以对应地看一下现实，推动工具快速进步的主要是人类永远无法满足的欲望。地球人口有七十亿，2004 年的时候，地球生态报告警告人类消耗已经超出了地球负荷。光是维持人类的食物供应，农药，化肥，杂交育种，辐射育种，很多技术在争议中快速发展。如果停下来，我们可能就无法供养人类的生活。在这一点上，我们本质和小说中巨大的生存压力并无区别。</p>
<h4 id="工具能力向人类逼近，直到超越人类"><a href="# 工具能力向人类逼近，直到超越人类" class="headerlink" title="工具能力向人类逼近，直到超越人类"></a>工具能力向人类逼近，直到超越人类 </h4><h4 id="工具处于支配地位"><a href="# 工具处于支配地位" class="headerlink" title="工具处于支配地位"></a> 工具处于支配地位 </h4><h4 id="所以必须严格限制出厂条件"><a href="# 所以必须严格限制出厂条件" class="headerlink" title="所以必须严格限制出厂条件"></a> 所以必须严格限制出厂条件 </h4><h4 id="必然产生矛盾"><a href="# 必然产生矛盾" class="headerlink" title="必然产生矛盾"></a> 必然产生矛盾 </h4><h3 id="人类为何同情仿生人"><a href="# 人类为何同情仿生人" class="headerlink" title="人类为何同情仿生人"></a> 人类为何同情仿生人 </h3><p> 约翰・罗尔斯在《正义论》里提出过一个重要的概念，叫 “无知之幕”。这个概念提出一种原则，来实现社会政策的正义公正。无知之幕的意思是，当选择社会政策的时候，每个选择的人都应该删除自己是什么身份地位出身的记忆，以 “无知” 的方式进行选择。比如选择应不应该保留黑人奴隶制度的时候，你不知道自己是黑人还是白人。选择买卖妇女是不是合法的时候，你不知道自己是男人还是女人。</p>
<p>这个无知之幕，立在一个基础上，就是 “我们” 拥有对等的能力和智慧，我们相信白人和黑人、男人和女人在幸福和痛苦上是平等的，一方感受到的东西，另一方也能感受到。所以我们能真切地 “同情” 到对方。你不能跟龙虾、跟带鱼一起披上无知之幕，因为你们在能力、智慧和感知上不可能平等。</p>
<p>但是当人类创造出来的工具越来越接近我们，在能力上越来越平等的时候，不管他们是 AI、机器人还是仿生人，我们总有一天能和他们一起披上无知之幕。当我们披上无知之幕，我们就能明白，我们对仿生人命运悲剧的同情，本质是希望避免自己有一天也遭遇同样的悲剧。</p>
<p>Q：为什么创造的工具外表要无限逼近人？？</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/01/14/quick-read-zhexue-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/14/quick-read-zhexue-1/" class="post-title-link" itemprop="url">查拉图斯特拉如是说（尼采）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-01-14 23:33:37" itemprop="dateCreated datePublished" datetime="2022-01-14T23:33:37+08:00">2022-01-14</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新於</span>
        <time title="修改時間：2022-01-16 23:24:27" itemprop="dateModified" datetime="2022-01-16T23:24:27+08:00">2022-01-16</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%B8%BB%E9%A2%98%E4%B9%A6%E7%B1%8D%E9%80%9F%E8%AF%BB-%E5%93%B2%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">主题书籍速读-哲学</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>弗里德里希・尼采（Friedrich Nietzsche, 1844-1900），德国哲学家、古典学家，19 世纪的天才思想家之一。尼采 24 岁当上大学教授，34 岁因病退休，44 岁精神崩溃，在 20 年的写作生涯中，创作了大量批判基督教和理性主义的著作（如《悲剧的诞生》《快乐的科学》《道德的谱系》《善恶的彼岸》《偶像的黄昏》《查拉图斯特拉如是说》《敌基督者》等），堪称西方哲学和宗教传统尖锐的批判者，对 20 世纪的思想史产生了深远的影响。</p>
<h3 id="查拉图斯特拉是谁？"><a href="# 查拉图斯特拉是谁？" class="headerlink" title="查拉图斯特拉是谁？"></a>查拉图斯特拉是谁？</h3><h4 id="历史上的查拉图斯特拉是古代波斯的一个先知，创立了一个宗教"><a href="# 历史上的查拉图斯特拉是古代波斯的一个先知，创立了一个宗教" class="headerlink" title="历史上的查拉图斯特拉是古代波斯的一个先知，创立了一个宗教"></a>历史上的查拉图斯特拉是古代波斯的一个先知，创立了一个宗教 </h4><p> 查拉图斯特拉这个波斯语的名字在希腊语里对应的是琐罗亚斯德，他创立的那个宗教在西方就叫作 “琐罗亚斯德教”，也被称为 “拜火教”。这种宗教曾经是波斯帝国的国教，在南北朝时期就传到了中国，被称为 “祆教”，或者 “火祆教”。</p>
<h4 id="尼采为什么借用查拉图斯特拉德名字来写这本书"><a href="# 尼采为什么借用查拉图斯特拉德名字来写这本书" class="headerlink" title="尼采为什么借用查拉图斯特拉德名字来写这本书"></a>尼采为什么借用查拉图斯特拉德名字来写这本书 </h4><p>(1) 尼采德代言人 -&gt; 尼采把自己看作先知，预示自己来带给人类思想文化的革命<br>(2) 批判时代的弊病 -&gt; 诊断西方文化的危机（当时东方主义，人们对东方有好感，因此采用东方来对照）<br>(3) 开辟新的思想 -&gt; 反对宗教的恶劣影响<br> 尼采认为，查拉图斯特拉创立的宗教虽然在东方，但是对西方哲学中的柏拉图主义和基督教都产生了深远的影响，开创了西方人关注道德和关注来世的传统，而这正是尼采要重点反对的，因此他在著作中复活了查拉图斯特拉，让这位创始人去反对由他开创的宗教给世界带来的恶劣影响。</p>
<p>这是尼采所有作品里唯一一部用戏剧化的方式呈现的，他记载了查拉图斯特拉的思想和谈话，也记录了这位先知精神上的成长，读起来很有点 “成长小说” 的味道。</p>
<h3 id="他都说了什么重要的事情？"><a href="# 他都说了什么重要的事情？" class="headerlink" title="他都说了什么重要的事情？"></a>他都说了什么重要的事情？</h3><h4 id="上帝死了：揭示虚无主义的真相（骆驼）"><a href="# 上帝死了：揭示虚无主义的真相（骆驼）" class="headerlink" title="上帝死了：揭示虚无主义的真相（骆驼）"></a>上帝死了：揭示虚无主义的真相（骆驼）</h4><p>尼采说上帝死了，并且是我们杀死了上帝，意思是随着自然科学、历史学、社会学、人类学、宗教学等等学科的发展，越来越多的人意识到上帝只不过是人类的发明，世界进入了一个世俗化的时代，或者用马克斯・韦伯（Maximilian Weber）的话说，世界被 “祛魅”（Disenchantment）了。而当人们意识到上帝只不过是人类的发明，没有真正的神圣性，更没有任何实质性的力量时，自然也就不再相信上帝了，这就相当于杀死了上帝。而且尼采认为，这个科学化和世俗化的进程看起来是不可逆转的，因此他会说，上帝彻底死了，“再也活不过来了”。</p>
<h4 id="超人：克服虚无主义的智慧（狮子）"><a href="# 超人：克服虚无主义的智慧（狮子）" class="headerlink" title="超人：克服虚无主义的智慧（狮子）"></a>超人：克服虚无主义的智慧（狮子）</h4><p>尼采高声向人们宣告：“所有的神都死了，现在我们要让超人活起来！”<br>尼采的 “超人” 学说非常有名，英语里的超人 superman 就是从翻译尼采的德文词 Übermensch 来的。</p>
<p>那尼采心目中的 “超人” 想要实现什么目标呢？他的目标只有一个，那就是尼采心目中一切生命体的本质：权力意志。在这里尼采受到了叔本华的影响，叔本华认为这个世界的本质是 “生存意志”，也就是要活下去的意志。尼采接受了意志作为世界的本质，但是反对叔本华说的生存意志，尼采认为权力意志才是更加根本的动机。一切生物，不管是植物还是动物，都以扩大或增加自己的力量作为生存的目标，这就是生命的意志本身。而那些强调谦卑、关注来世的哲学和宗教，都是对生命本身的否定。所以尼采的 “超人” 学说，就是让人关注身体、关注大地、关注今生，现实地提高自己在各方面的力量，而不是把希望寄托在道德这种内在的要求，或者天国、来生之类虚幻的前景上。</p>
<p>理解了尼采的 “超人” 学说，我们也就可以理解《查拉图斯特拉如是说》那个谜一般的副标题了。尼采为什么说这本书既是写给所有人的，又不是写给任何人的呢？说它 “为所有人而写”，是因为只要一个人能够听懂尼采的教导，走上自我超越的道路，这本书就是为他而写的，这是对每个人都成立的人生理想。说它 “不为任何人而写”，是因为尼采主张的这种自我超越，说起来容易，做起来却很难。</p>
<h4 id="永恒轮回：拥抱永恒轮回的超人（小孩）"><a href="# 永恒轮回：拥抱永恒轮回的超人（小孩）" class="headerlink" title="永恒轮回：拥抱永恒轮回的超人（小孩）"></a>永恒轮回：拥抱永恒轮回的超人（小孩）</h4><p>“永恒轮回” 带来了比 “上帝之死” 更大的虚无，给前面说的通过自我超越创造意义的学说狠狠地泼了一盆冷水，把那个努力超越自己的 “超人” 重新推回到了虚无之中。认识到永恒轮回，让尼采笔下的查拉图斯特拉也经历了一场严重的精神危机。他整整三天不吃、不喝、不睡，也不跟人说话。</p>
<p>查拉图斯特拉精神上的最终成熟，正是通过这场精神危机实现的。经过长时间的思考，他转换了视角，把永恒轮回看作检验自己是否真的接受了 “超人” 学说的试金石，因为真正的 “超人” 就是要坦然接受永恒轮回，甚至要热情拥抱这个可怕的真理，然后将它也一起克服掉。真正的 “超人” 明明知道整个世界是被决定的，一切都会重演，但是依然勇敢坚毅地生活下去，依然努力地超越自己，拥抱在每一个当下实现出来的美好，每一个当下展现出来的力量。这才是尼采心目中，人对虚无主义取得的最终胜利。</p>
<p>加缪的小说《西西弗斯的神话》。西西弗斯因为冒犯了诸神，被罚每天推一块大石头上山，他每次费尽力气把石头推山上之后，石头就会自己滚下来，然后西西弗斯就要再次把它推山上，周而复始。这个驴拉磨一样的工作看起来毫无意义，就像尼采说的永恒轮回。但是在加缪笔下，西西弗斯勇敢地面对命运的荒谬，为自己赢得了意义，这正是尼采那里真正的 “超人” 所做的事情。</p>
<h3 id="影响"><a href="# 影响" class="headerlink" title="影响"></a>影响 </h3><p> 在哲学方面，尼采被称为 “存在主义之前的存在主义者”，他对虚无主义和人生意义的深刻反思，启发了海德格尔、雅思贝尔斯、加缪这些存在主义者。进入 20 世纪六七十年代，尼采又在法国思想界掀起了第二波的浪潮，形成了一个所谓的 “法国尼采学派”，巴塔耶、福柯、德勒兹、德里达都是《查拉图斯特拉如是说》的深度粉丝，他们从里面发展出了反本质主义、反理性主义、解构主义的思想。</p>
<p>在心理学方面，19 世纪末到 20 世纪上半叶最重要的三位心理学家弗洛伊德、阿德勒和荣格都深受《查拉图斯特拉如是说》的影响，荣格还在好几年里专门开课讲过这本书。他们看到了尼采对人类心理细致入微的观察，还有对人类心灵力量的肯定。</p>
<p>《查拉图斯特拉如是说》对当代文学和艺术的影响更是惊人的，它被誉为 19 世纪末 20 世纪初 “新艺术运动” 的 “圣经”。这本书里展现的革命精神，解放了人类心灵里面的隐秘力量，让艺术家们大胆地反对 19 世纪的现实主义和自然主义，开启了象征主义、表现主义、意识流、蒙太奇等等一系列的文学艺术观念，诗人霍夫曼施塔尔、里尔克、叶芝，小说家托马斯・曼、黑塞、纪德、乔伊斯、普鲁斯特，剧作家布莱希特、萧伯纳，作曲家马勒和理查・施特劳斯都深受它的影响。特别值得一提的是，理查・施特劳斯创作了题为《查拉图斯特拉如是说》的交响诗，这部交响诗的开篇非常著名，用辉煌灿烂的音响表现了旭日东升，查拉图斯特拉离开自己的山洞，与太阳对话的场景，听起来让人有种灵魂飞升的震撼。这段音乐也被用在了很多影视作品里，其中最有名的大概就是库布里克的电影《2001 太空漫游》了。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/01/14/quick-read-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/14/quick-read-5/" class="post-title-link" itemprop="url">quick-read-5</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-01-14 23:33:13" itemprop="dateCreated datePublished" datetime="2022-01-14T23:33:13+08:00">2022-01-14</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新於</span>
        <time title="修改時間：2022-01-16 23:11:24" itemprop="dateModified" datetime="2022-01-16T23:11:24+08:00">2022-01-16</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/01/14/quick-read-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/14/quick-read-4/" class="post-title-link" itemprop="url">平行宇宙（加来道雄）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>
      

      <time title="創建時間：2022-01-14 13:20:17 / 修改時間：13:45:25" itemprop="dateCreated datePublished" datetime="2022-01-14T13:20:17+08:00">2022-01-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%B8%BB%E9%A2%98%E4%B9%A6%E7%B1%8D%E9%80%9F%E8%AF%BB-%E5%A4%A9%E6%96%87%E5%92%8C%E7%89%A9%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">主题书籍速读-天文和物理</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="M- 理论是如何从无到有，一步步地发展成人类最前沿物理学理论的"><a href="#M- 理论是如何从无到有，一步步地发展成人类最前沿物理学理论的" class="headerlink" title="M 理论是如何从无到有，一步步地发展成人类最前沿物理学理论的"></a>M 理论是如何从无到有，一步步地发展成人类最前沿物理学理论的 </h3><h4 id="弱电理论 - 标准模型"><a href="# 弱电理论 - 标准模型" class="headerlink" title="弱电理论 标准模型"></a> 弱电理论 标准模型 </h4><p> 虽然爱因斯坦没能构建起万物理论，但他为物理学界打开了视野，所以后来的学者都能朝这条路努力。比如 20 世纪 60 年代，美国物理学家史蒂文・温伯格等人成功地把四种基本力中的两种，也就是把弱核力和电磁力，用弱电理论统一起来，他们也因此获得了诺贝尔物理学奖。</p>
<p>7 年之后，科学家们又进一步，他们提出了一个叫做 “标准模型” 的新理论，这个理论统一了电磁力、弱核力和强核力，也就是说除了引力之外，统一了四种力中的三种。到目前为止，标准模型能满足所有关于粒子物理学的实验数据，按理来说也算是挺成功了，但科学家们却很不满意，因为标准模型有很大问题，那就是它本身特别奇怪。比如，为了符合实验数据，模型里有人为放进去的 19 个参数，而这些参数本身没有任何理由和意义，所以这个模型看起来就像是修修补补拼凑出来的。再加上标准模型并没有把引力统一进去，所以它也不是科学家们想要追求的万物理论。</p>
<h4 id="弦论"><a href="# 弦论" class="headerlink" title="弦论"></a>弦论 </h4><p> 同样是 20 世纪 60 年代，意大利物理学家韦内齐亚诺和同事创立了一种叫做弦论的物理理论，就是我们刚才提到的弦论。为什么叫弦论呢？因为这个理论认为，所有的微观粒子，比如电子和中微子，本质上都是空间中一根根微小的弦通过振动产生的，因为弦具有不同的振动状态，所以粒子才会出现不同的种类。这个弦有多小呢？科学家们规定，它的长度是物理学能允许存在的最小长度，用术语说就是一个普朗克长度，大约有 $1.6 \times 10^{-33}$ 厘米这么小。</p>
<h4 id="M- 理论"><a href="#M- 理论" class="headerlink" title="M 理论"></a>M 理论 </h4><p> 科学家们就发现了弦论的形式其实不止有一种，而是多达五种，这五种形式不同的弦论都自成一体，能自圆其说。这就让科学家们很困惑，他们没法解释为什么宇宙中会有五种弦论，再加上其他种种问题，科学家们逐渐意识到弦论还有着重大的理论缺陷。</p>
<p>直到 1994 年，美国物理学家 Edward Witten 等人才从数学角度做出了重大突破。他们发现，我们其实可以用一种更包罗万象的理论来把五种不同的弦论统一在一起，而这种包罗万象的理论就是 M 理论。M 理论的突破之处就在于，它可以利用数学方法，用五种方式来演算，变幻成五种不同的弦论，所以它实质上就是五种弦论的统一体，这样就能解决弦论的理论缺陷了。</p>
<h3 id="M- 理论和其他各种推论是怎么支持平行宇宙存在猜想的"><a href="#M- 理论和其他各种推论是怎么支持平行宇宙存在猜想的" class="headerlink" title="M 理论和其他各种推论是怎么支持平行宇宙存在猜想的"></a>M 理论和其他各种推论是怎么支持平行宇宙存在猜想的 </h3><h4 id="M- 理论的数百万个方程解"><a href="#M- 理论的数百万个方程解" class="headerlink" title="M 理论的数百万个方程解"></a>M 理论的数百万个方程解</h4><p> 所以有越来越多的科学家认为，如果我们没有办法给弦论的方程式找到一个独一无二的解，那干脆就别费这个劲儿了，因为所有的解可能都是对的，每个解都对应着一个平行宇宙。</p>
<h4 id="暗物质"><a href="# 暗物质" class="headerlink" title="暗物质"></a>暗物质 </h4><p> 因为 M 理论推导出，引力可以穿越时空的界限，抵达其他的宇宙空间，也就是说，平行宇宙中的星系产生的引力，可以抵达我们的宇宙，从而被我们观测到，所以我们虽然看不到其他宇宙中的星系，但是却能通过观察它们产生的引力，间接地发现它们的存在，这就是暗物质的来源。</p>
<h4 id="Andrei-Linde"><a href="#Andrei-Linde" class="headerlink" title="Andrei Linde"></a>Andrei Linde</h4><p>从哲学的角度来看，如果我们认为宇宙是通过某种过程产生的，那这种过程就没理由只发生一次，所以作者说，只要我们承认一个宇宙可以被创造出来，那我们就几乎要被迫承认，无限多个平行宇宙也可以被创造出来。</p>
<h4 id="Martin-John-Rees"><a href="#Martin-John-Rees" class="headerlink" title="Martin John Rees"></a>Martin John Rees</h4><p>生命的诞生条件是很苛刻的，而我们的宇宙恰好满足能产生生命的所有条件，那这就不应该只是一个巧合，我们也不应该把它当成一个巧合。要想解释这个现象，就最好假定存在着无数个宇宙，在这么多宇宙里，大多数都不具备产生生命的条件。</p>
<h3 id="为什么说平行宇宙很可能是人类的最终归宿"><a href="# 为什么说平行宇宙很可能是人类的最终归宿" class="headerlink" title="为什么说平行宇宙很可能是人类的最终归宿"></a>为什么说平行宇宙很可能是人类的最终归宿 </h3><h4 id="世界末日"><a href="# 世界末日" class="headerlink" title="世界末日"></a> 世界末日 </h4><p> 目前，宇宙大爆炸理论，是人类所有关于宇宙演化的理论中最被科学家们认可的一个。大爆炸理论猜想的是，宇宙是从一个叫做奇点的无限小的点爆炸而来的，奇点爆炸之后就一直在不断膨胀，一直过了 137 亿年才形成了今天我们所看到的宇宙，而且这个膨胀的过程一直还在持续着。目前科学家们还没法确定，这个膨胀的进程是会一直持续下去，还是有一天会停止。</p>
<p>世界末日可能会有两种形式：第一种是，宇宙会像现在一样继续膨胀，而且无限膨胀下去，最后所有生物灭亡；<br>(1) 宇宙中所有发光发热的恒星，我们可以把它们类比成煤球，虽然可以点着，但总有一天会熄灭<br>(2) 热力学第二定律 -&gt; 降温</p>
<p>第二种是，宇宙有一天会停止膨胀，然后开始收缩，进行一个大爆炸的反向过程，最后宇宙里所有的物质都会收缩到一个点上，然后同样的，所有生物都会灭亡。</p>
<h4 id="哪些方法延续文明"><a href="# 哪些方法延续文明" class="headerlink" title="哪些方法延续文明"></a>哪些方法延续文明 </h4><p> 俄罗斯物理学家尼古拉・卡尔达舍夫，曾经就提出过宇宙中智慧文明等级的分类方法，他把文明分成三等，用罗马数字的 Ⅰ、Ⅱ、Ⅲ 来标记。Ⅰ 类文明指的是能掌握行星级别能量的文明；Ⅱ 类文明则可以掌握一颗恒星的能量；Ⅲ 类文明指的是能掌握一整个星系所包含能量的文明。每类文明和更低一等级文明之间的差距约为 100 亿倍。人类文明目前还达不到 Ⅰ 类文明的水平，因为我们还不能随心所欲地掌握地球上的所有能量。科学家们估算，人类文明目前只能算是 0.7 类文明，而只有达到 Ⅲ 类文明时，我们可能才会拥有逃离这个宇宙的能力。</p>
<p>先让我们乐观地假设，人类还可以安全快速地发展很长很长时间，最终达到 Ⅲ 类文明，那如果有一天未来人类铁了心，要趁宇宙灭亡前离开我们这个宇宙，那还必须经过一系列的步骤。</p>
<p>第一步，未来人类要创立一个万物理论。这个理论肯定要能解释宇宙中一切的基本现象，然后还需要用先进技术对这个理论进行验证，这样他们就能弄清楚很多方案的可行性了，比如怎么打开通往另一个宇宙的时空隧道。第二步，进入时空隧道或者制造一个新宇宙。未来人类可以通过先进的技术找到宇宙中自然存在的时空隧道，甚至可以自己制造出来时空隧道，这样就能前往其他的平行宇宙。或者人类也可以自己创造一个平行宇宙，因为到那时候，人类说不定就掌握了主动引发宇宙大爆炸的能力，可以在自己身边再造一个宇宙。第三步，如果刚刚说的所有路都被堵死，未来人类发现自己确实没有办法进行宇宙间的穿越，那唯一的出路可能就是再造文明了。霍金认为，根据量子理论的原理，如果物体的尺寸足够小的话，那这个物体就有可能进行时空穿越。如果未来人类能把自己的文明信息放到一个特别小的载体上，那或许就能让自己文明的信息穿越到另一个宇宙空间，这样一来，承载着人类文明种子的载体，也可以肩负起重担，在平行宇宙中再造人类文明。</p>
<p>Q：为什么想延续文明，这是哪种哲学思想？</p>
<p>美国物理学家 Steven Weinberg 曾经说过，“人生是场闹剧，只有为数不多的几件事可以让人生变得略有意义，努力认识宇宙便是其中之一，可以让人生带上一点悲剧性的色彩。” 中国的苏轼也有云，“寄蜉蝣于天地，渺沧海之一粟。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://wangxchun.github.io/2022/01/13/quick-read-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Nathan Wang">
      <meta itemprop="description" content="機器學習 / C++ / 投資 / 台灣景點">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nathan自强不息">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/13/quick-read-3/" class="post-title-link" itemprop="url">黑洞简史（Marcia Bartusiak）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-01-13 23:53:31" itemprop="dateCreated datePublished" datetime="2022-01-13T23:53:31+08:00">2022-01-13</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新於</span>
        <time title="修改時間：2022-01-14 13:32:59" itemprop="dateModified" datetime="2022-01-14T13:32:59+08:00">2022-01-14</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%B8%BB%E9%A2%98%E4%B9%A6%E7%B1%8D%E9%80%9F%E8%AF%BB-%E5%A4%A9%E6%96%87%E5%92%8C%E7%89%A9%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">主题书籍速读-天文和物理</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>黑洞理论和广义相对论的关系？</p>
<h3 id="黑洞理论发展"><a href="# 黑洞理论发展" class="headerlink" title="黑洞理论发展"></a>黑洞理论发展 </h3><p> 米歇尔：恒星质量够大，会把发射的光吸收回去，变成黑洞 -&gt; 被忽略</p>
<p>随着广义相对论的提出，爱因斯坦发现恒星会将光向自身的方向吸引。既然光线会被恒星吸引过来，德国的物理学家史瓦西就做出了一个大胆的猜想：如果有一个恒星的质量特别大，以至于它不仅仅是将光线 “稍微向自己弯曲”，而是把光线完全吸引到自己的势力范围内，从而使光线根本没有办法逃出去，那么这颗恒星不仅不会被外界看到，还会变成一个 “无底洞”，光和任何物质都只能进去，根本没有办法出来。-&gt; 被质疑</p>
<p>钱德勒・塞卡和奥本海默这两个重量级的物理学大师分别通过计算发现，恒星的质量如果足够大，那它在死亡的时候，会由于自身的引力过大，而不断地向内塌陷，最终形成一个质量无限大、体积无限小的黑洞。-&gt; 被攻击</p>
<p>到了 20 世纪 60 年代，随着射电望远镜的应用，物理学家第一次发现宇宙中居然真的存在密度比中子星还要大的天体时，才开始关注到黑洞的相关理论，并最终接受了黑洞的概念。-&gt; 被接收</p>
<p>-&gt; 在 1929 年的股市大跌之前，他将自己的所有资金转移到了安全的地方，这让他作为美国最富有的人之一。</p>
<h3 id="如何看到黑洞"><a href="# 如何看到黑洞" class="headerlink" title="如何看到黑洞"></a>如何看到黑洞 </h3><h4 id="X- 射线"><a href="#X- 射线" class="headerlink" title="X 射线"></a>X 射线</h4><p> 捕捉黑洞在吞食恒星过程中向外发射出的 X 射线</p>
<p>恒星的物质在靠近黑洞的时候，并不是直接掉进去，而是由于黑洞的旋转，也开始旋转起来，并且越往黑洞靠拢，旋转的速度也越大，以至于在靠近黑洞视界的时候，物质旋转的速度会接近光速。如果这些旋转的物质受到了某种干扰，它就有机会从垂直的方向被发射出去。黑洞附近的干扰确实太多了，比如黑洞强大的磁场就是这样的一种干扰源。当一个黑洞在吞食其他恒星的时候，会由于自身的磁场而发射出大量 X 射线。正是通过这样的方式，科学家 “看到” 了黑洞。</p>
<h4 id="引力波"><a href="# 引力波" class="headerlink" title="引力波"></a>引力波</h4><p>2016 年的时候，科学家发布消息，人类第一次探测到引力波的存在，这样的引力波是由两个黑洞合并造成的。</p>
<p>随着广义相对论的提出，爱因斯坦发现恒星会将光向自身的方向吸引。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一頁"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一頁"></i></a>
  </nav>


<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nathan Wang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 強力驅動
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  



<script>
if (document.querySelectorAll('.pdf-container').length) {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/pdfobject@2.2.4/pdfobject.min.js', () => {
    document.querySelectorAll('.pdf-container').forEach(element => {
      PDFObject.embed(element.dataset.target, element, {
        pdfOpenParams: {
          navpanes : 0,
          toolbar  : 0,
          statusbar: 0,
          pagemode : 'thumbs',
          view     : 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height   : element.dataset.height
      });
    });
  }, window.PDFObject);
}
</script>



  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
